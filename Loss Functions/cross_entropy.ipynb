{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6563317",
   "metadata": {},
   "source": [
    "# Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc6e36",
   "metadata": {},
   "source": [
    "### Theoretical Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b653c5",
   "metadata": {},
   "source": [
    "- We previously went through the idea of Shannon Entropy and Kullbeck-Leibler Divergence. The content below assumes full knowledge of that notebook. So if anything doens't make sense, go read the KL divergence notebook and/or the Shannon Entropy notebook for a deeper explanation.\n",
    "\n",
    "- We know that, for a probability distribution $p(x)$, its Shannon Entropy (expected number of bits needed to encode the distribution) is defined as\n",
    "$$\\begin{aligned}\n",
    "    H(p) &= - \\sum_i p(x_i) \\cdot \\log_2(p(x_i))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We further know that, if we encode $p(x)$ inappropriately with a distribution $q(x)$, the \"uncertainty\" gained from this encoding (i.e. the increase in the expected number of bits needed to express $p(x)$) may be defined as its KL divergence\n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(p || q) &= \\sum_i p(x_i) \\cdot \\log_2(p(x_i)) - \\sum_i p(x_i) \\cdot \\log_2(q(x_i)) \\\\\n",
    "    &= - H(p) - \\sum_i p(x_i) \\cdot \\log_2(q(x_i))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Cross Entropy between distributions $p$ and $q$ is then simply defined as\n",
    "$$\\begin{aligned}\n",
    "    H(p,q) &= - \\sum_i p(x_i) \\cdot \\log_2(q(x_i))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Taken together, the relationship between the KL divergence, Shannon Entropy, and Cross Entropy can be written as\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    D_{KL}(p || q) &= - H(p) + H(p,q) \\\\\n",
    "    \\text{KL Divergence} &= - \\text{Shannon Entropy} + \\text{Cross Entropy}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b578594",
   "metadata": {},
   "source": [
    "### Why is cross entropy important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f0adf",
   "metadata": {},
   "source": [
    "- Cross entropy is particularly important in deep learning as a loss function to measure the distance between 2 probability distributions easily\n",
    "    - In classification problems, it is typically used together with softmax to compute the distance between a network's internal representation with the actual labels. We will see why in the subsection below on `Differentiabiliy of Cross Entropy`\n",
    "    - This can be used whether the labels are one hot encoded, or if the labels are a probability distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f8e3b",
   "metadata": {},
   "source": [
    "### Differentiability of Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e3fb2",
   "metadata": {},
   "source": [
    "- Loss functions must be differentiable. Let's see how Cross Entropy can be differentiated\n",
    "\n",
    "- Note that in the case where labels are one-hot encoded, cross entropy becomes the **negative log likelihood**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\mathcal{L} &= - \\sum_i y_i \\log(p_i) \\\\\n",
    "    &= -\\log(p_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Therefore\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial p_i} &= \\frac{\\partial}{\\partial p_i} (-\\log(p_i)) \\\\\n",
    "    &= -\\frac{1}{p_i}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5e1ec",
   "metadata": {},
   "source": [
    "#### Why Cross Entropy is almost always used alongside Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32969131",
   "metadata": {},
   "source": [
    "- Recall from the notebook `softmax.ipynb` that the derivative of softmax is given by\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial x_i}{\\partial z_j} &= \\begin{cases}\n",
    "        x_i \\cdot (1 - x_i) & \\forall i==j \\\\\n",
    "        -x_i \\cdot x_j & \\forall i \\neq j \\\\\n",
    "    \\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Remember our notation here; $x_i$ is the $i$-th softmax value, $z_i$ is the $i$-th input value \n",
    "\n",
    "- Now, suppose we have this basic network output: `layer output --> softmax --> cross entropy`. How should we do backpropagation?\n",
    "\n",
    "- This is effectively asking, how do we compute the change in the loss for a given change in our layer output $z_i$;\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial z_j} &= \\frac{\\partial \\mathcal{L}}{\\partial x_i} \\cdot \\frac{\\partial x_i}{\\partial z_j} \\\\\n",
    "    &= \\begin{cases}\n",
    "        -\\frac{1}{x_i} \\cdot x_i \\cdot (1 - x_i) & \\forall i==j \\\\\n",
    "        -\\frac{1}{x_i} \\cdot -x_i \\cdot x_j & \\forall i \\neq j \\\\\n",
    "    \\end{cases} \\\\\n",
    "    &= \\begin{cases}\n",
    "        x_i - 1 & \\forall i==j \\\\\n",
    "        x_j & \\forall i \\neq j \\\\\n",
    "    \\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- The terms cancel out, which means we can compute our gradients using either the softmax output - 1, or the softmax output itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc4de0",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151a0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9163) 0.9162907\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def my_cross_entropy(input_arr: np.ndarray, labels: np.ndarray) -> float:\n",
    "    rescaled_input_arr = input_arr + 1e-6\n",
    "    rescaled_input_arr /= np.sum(rescaled_input_arr)\n",
    "    \n",
    "    return -np.sum(np.log(input_arr) * labels)\n",
    "\n",
    "logits = torch.tensor([[0.25, 0.35, 0.4]], dtype=torch.float32)  \n",
    "labels = torch.tensor([2]) #index of positive label\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = -(labels == torch.arange(3)).float() * torch.log(logits)\n",
    "\n",
    "print(loss.sum(), my_cross_entropy(logits.numpy(), (labels==torch.arange(3)).numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
