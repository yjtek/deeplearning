{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de87ca91",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79e07f",
   "metadata": {},
   "source": [
    "- Softmax is a technique to constraint an array of values to sum to 1\n",
    "\n",
    "- This is heavily used in deep learning, because we want this property to interpret the output values from networks as probabilities\n",
    "    - Does softmaxxing necessarily make an array of numbers a \"probability\"? Not always, you can have miscalibration\n",
    "\n",
    "- Softmax is embarassingly simple; just take the exponentiated value and divide by the exponentiate sum\n",
    "$$\\begin{aligned}\n",
    "    \\text{softmax}(z_i) &= \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Advantages\n",
    "    - Exponentiation forces separation; large values get disproportionately large probabilities\n",
    "    - Guaranteed to sum to 1\n",
    "    - Differentiable! See section on **differentiability of softmax** \n",
    "\n",
    "\n",
    "- Disadvantages\n",
    "    - Exponentiation makes softmax scale-sensitive. So if you have an array [1,2,3] vs [1000,2000,3000], softmax produces wildly different results\n",
    "        - To resolve this, the arrays are often preprocess by subtracting the maximum value \n",
    "    - Can be very expensive to compute when vocabulary size is large. See section on **Reducing Computational Cost of Softmax**\n",
    "\n",
    "- **NOTE:** Softmax is particularly noteworthy because it is frequently paired with a very basic loss function - cross entropy - because when they are taken together the partial derivative becomes ridiculously simple. See section on `cross_entropy` for details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8e452",
   "metadata": {},
   "source": [
    "### Differentiability of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8d1d1",
   "metadata": {},
   "source": [
    "- Remember, softmax converts a numeric array of size $N$ to another array of size $N$ such that they sum to 1\n",
    "\n",
    "- Therefore, every value in the output array $x_i$, is influenced by every value in the input array $z_i$, since all values of $z$ appear in the denominator minimally\n",
    "\n",
    "- Thus, there are 2 cases to consider for softmax differentiation. We'll put down the answer, the derivation of both cases are below\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial x_i}{\\partial z_j} &= \\begin{cases}\n",
    "        x_i \\cdot (1 - x_i) & \\forall i==j \\\\\n",
    "        -x_i \\cdot x_j & \\forall i \\neq j \\\\\n",
    "    \\end{cases}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b93be8",
   "metadata": {},
   "source": [
    "- For $\\frac{\\partial x_i}{\\partial z_j}$ where $i==j$\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial x_i}{\\partial z_i} &= \\frac{\\partial}{\\partial z_i} (\\frac{e^{z_i}}{\\sum_k e^{z_k}}) \\\\\n",
    "    &= \\frac{\\sum_k e^{z_k} \\cdot e^{z_i} - e^{2 z_i}}{(\\sum_k e^{z_k})^2} \\\\\n",
    "    &= \\frac{e^{z_i}[\\sum_k e^{z_k} - e^{z_i}]}{(\\sum_k e^{z_k})^2} \\\\\n",
    "    &= \\frac{e^{z_i}}{\\sum_k e^{z_k}} \\cdot \\frac{\\sum_k e^{z_k} - e^{z_i}}{\\sum_k e^{z_k}} \\\\\n",
    "    &= x_i \\cdot (1 - x_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- For $\\frac{\\partial x_i}{\\partial z_j}$ where $i \\neq j$\n",
    "$$\\begin{aligned}\n",
    "    \\frac{\\partial x_i}{\\partial z_j} &= \\frac{\\partial}{\\partial z_j}(\\frac{e^{z_i}}{\\sum_k e^{z_k}}) \\\\\n",
    "    &= \\frac{0 - (e^{z_i} e^{z_j})}{(\\sum_k e^{z_k})^2} \\\\\n",
    "    &= - \\frac{e^{z_i}}{\\sum_k e^{z_k}} \\frac{e^{z_j}}{\\sum_k e^{z_k}} \\\\\n",
    "    &= - x_i \\cdot x_j\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aba8a0",
   "metadata": {},
   "source": [
    "### Implementation of Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82236f",
   "metadata": {},
   "source": [
    "- Super straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5155ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "arr = np.random.rand(20)\n",
    "\n",
    "def yj_softmax(input_arr: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(input_arr) / np.sum(np.exp(input_arr))\n",
    "\n",
    "np.testing.assert_array_almost_equal(yj_softmax(arr), softmax(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79be3f5",
   "metadata": {},
   "source": [
    "### Reducing Computational Cost of Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d614199",
   "metadata": {},
   "source": [
    "- One drawback of this loss is that the softmax itself can be expensive to compute\n",
    "\n",
    "- BERT's vocabulary size is 30-50k words. And we have to perform this softmax over this for each masking index. If you corpus has 1M tokens, that means ~150k softmax must be computed. \n",
    "\n",
    "- While this is manageable on modern GPUs, in some contexts, it can easily be overwhelm your compute. To overcome these there are a few tricks that people have used. Though computational power has largely increased to a point where these are not super relevant, they are nevertheless instructive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35520e1a",
   "metadata": {},
   "source": [
    "#### 1. Adaptive Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feacc42",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953639c",
   "metadata": {},
   "source": [
    "#### 2. Sampled Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67583a17",
   "metadata": {},
   "source": [
    "#### 3. Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf043ce",
   "metadata": {},
   "source": [
    "#### 4. Sparse Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9faa9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
